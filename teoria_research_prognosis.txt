Fundamentos Teóricos del Sistema de Prognosis Industrial
Juan David Valencia Piedrahita
Noviembre 2025

1.

Introducción

Este documento complementa el artículo principal describiendo con rigor doctoral las bases teóricas
detrás de cada componente del sistema de prognosis industrial. Se abordan técnicas de gestión de
datos, normalización estadística, métricas de priorización, modelos temporales, inferencia probabilística
y aprendizaje incremental, resaltando sus fundamentos científicos y las razones de diseño que guían su
implementación.

2.

Gestión e Integridad de los Datos

2.1.

Normalización de Nombres mediante Unicode

La normalización Unicode en forma NFKD descompone cada punto de código en sus componentes
canónicos, separando letras de diacríticos y sustituyendo símbolos compuestos por secuencias base. El
Consorcio Unicode [?] demuestra que este proceso garantiza la equivalencia semántica entre representaciones múltiples de la misma cadena. En entornos industriales, donde un analizador puede alternar
idiomas o codificaciones, aplicar NFKD seguido de filtrado de diacríticos elimina ambigüedades sintácticas y permite construir catálogos de sensores inmutables. La consistencia nominal es crucial para
mapear mediciones a clases del pipeline y para asegurar que los metadatos almacenados en la base de
datos no dependan del formato de exportación del equipo.

2.2.

pandas como Motor de Perfilamiento

La biblioteca pandas ofrece estructuras tabulares (DataFrame) que encapsulan valores y metadatos,
soportando operaciones vectorizadas sobre columnas completas. Su diseño, documentado por McKinney
y la comunidad de código abierto, permite aplicar funciones de agregación, detección de valores atípicos
por cuantiles y generación de reportes exploratorios en un solo paso declarativo. El uso de pandas en
la fase de ingesta asegura reproducibilidad, dado que cada transformación queda registrada como
un conjunto de operaciones deterministas sobre la columna original. Asimismo, la integración con
serializadores como Parquet o CSV facilita trazar el linaje desde el archivo bruto hasta el dataset
normalizado.

2.3.

Persistencia Transaccional

Los principios ACID descritos por Gray y Reuter [?] garantizan atomicidad, consistencia, aislamiento
y durabilidad en bases de datos relacionales. Al persistir los lotes de ingesta en PostgreSQL bajo
transacciones explícitas, se asegura que los conjuntos normalizados, los parámetros de escalamiento y
los registros de auditoría se escriban como una unidad indivisible. Ante fallos de red o interrupciones

1

eléctricas, el motor revierte la transacción completa, evitando estados parciales que comprometerían
la trazabilidad del pipeline MLOps.

3.

Normalización Estadística y Reproducibilidad

3.1.

Estandarización Z-Score con StandardScaler

La estandarización Z-score transforma una variable x en z = (x − µ)/σ, donde µ y σ representan la
media y desviación estándar del conjunto de entrenamiento. Esta transformación produce variables con
media cero y varianza unitaria, permitiendo comparar magnitudes expresadas en escalas heterogéneas.
El estimador implementado en StandardScaler [?] conserva los parámetros ajustados en la fase de
entrenamiento para reutilizarlos sobre lotes futuros. La reproducibilidad se deriva de registrar dichos
parámetros en una bitácora versionada; así, cualquier análisis posterior puede reconstruir el mismo
espacio normalizado y validar que no existe contaminación de datos (data leakage). A nivel estadístico,
el Z-score favorece algoritmos sensibles a la escala (p.ej., regresiones penalizadas, SVM) y evita que las
variables de mayor rango dominen la varianza total.

4.

Métricas de Priorización de Variables

4.1.

Varianza Normalizada (SV ar )

La varianza captura la dispersión de una variable respecto a su media. Sin embargo, cuando se comparan
sensores con distribuciones distintas, es necesario ajustar la varianza por la proporción de valores
únicos y por un término de regularización que evite singularidades. El indicador SV ar = log(var +ϵ) · r
privilegia señales dinámicas con suficiente soporte estadístico, alineado con los principios de selección
de características propuestos en la estadística multivariante [?].

4.2.

Estabilidad Temporal (SEstab )

SEstab compara la desviación estándar global con la desviación de ventanas móviles. Variables útiles
deben exhibir variaciones suaves y predecibles; de lo contrario, los modelos se vuelven sensibles al ruido.
Esta métrica materializa el principio de control estadístico de procesos: premiar comportamientos con
autocorrelación interpretable y penalizar señales erráticas que inducen falsos positivos.

4.3.

Principio de Equilibrio entre Variabilidad y Estabilidad

La selección de sensores requiere balancear la capacidad de detectar cambios (variabilidad) con la
facilidad de interpretación (estabilidad). Desde la teoría de control estadístico, una variable completamente plana no aporta información, mientras que una con alta varianza y sin estructura dificulta la
generación de alertas accionables. Los indicadores SV ar y SEstab se complementan para mantener este
equilibrio, asegurando que cada variable crítica aporte diversidad sin sacrificar gobernabilidad.

4.4.

Percentiles Dinámicos

Los percentiles dinámicos recalculan los umbrales sobre cada lote, evitando el sesgo de límites fijos
cuando cambian las distribuciones. Esta técnica, relacionada con los control charts adaptativos, permite
clasificar variables en categorías (crítica, monitoreo, auditoría) aun cuando la variabilidad global se
expanda o contraiga. Matemáticamente, los percentiles se estiman sobre la distribución puntual del
score ponderado, preservando el orden relativo de importancia.
2

4.5.

Correlación Sistémica (SCorr )

SCorr promedia las correlaciones absolutas de un sensor frente al resto del sistema. Una correlación
elevada sugiere que la variable participa activamente en la dinámica eléctrica y puede funcionar como
síntoma anticipado de fallas. Este indicador se inspira en la teoría de grafos correlacionales y en
el análisis de componentes principales, donde las variables altamente conectadas explican porciones
significativas de la varianza conjunta.

5.

Modelado Temporal y Ensemble Híbrido

5.1.

SARIMAX y Pruebas ADF/KPSS

El modelo SARIMAX (SARIMA(p, d, q)(P, D, Q)s con regresores exógenos) captura dependencias autorregresivas, integradas y de medias móviles tanto en la componente regular como en la estacional [?].
Los órdenes (p, d, q) definen la dinámica de corto plazo, mientras que (P, D, Q)s describe patrones que
se repiten cada s períodos (p.ej., 24 horas). La identificación de d y D requiere verificar la estacionariedad: la prueba Augmented Dickey-Fuller (ADF) contrasta la hipótesis de raíz unitaria, mientras que
KPSS evalúa la hipótesis de estacionariedad. Solo cuando ambas evidencian estacionariedad se procede
a ajustar el modelo; de lo contrario, se aplican diferencias o transformaciones Box-Cox.

5.2.

Prophet

Prophet [?] es un modelo aditivo que combina tendencia g(t), estacionalidad s(t) y efectos de vacaciones
h(t). Utiliza funciones de base de Fourier para capturar estacionalidades múltiples y detecta automáticamente puntos de cambio mediante priors logísticos. Su formulación genera predicciones robustas
ante datos faltantes y cambios estructurales, complementando a SARIMAX cuando la serie presenta
comportamientos no lineales o eventos externos.

5.3.

Isolation Forest y Ratio Promedio de Anomalías

Isolation Forest [?] aísla observaciones mediante particiones aleatorias; las instancias anómalas requieren menos divisiones para quedar aisladas. El ratio promedio reportado (porcentaje de muestras
marcadas como anomalías) sirve como indicador de salud del modelo base. Valores entre 3–5 % señalan
que la mayor parte de las observaciones concuerdan con la línea normal; incrementos sostenidos reflejan
desviaciones sistémicas o concept drift y justifican reentrenamientos.

6.

Métricas de Desempeño y Diagnóstico

6.1.

RMSE y MAE

El Error Cuadrático Medio (RMSE) penaliza fuertemente las desviaciones grandes al elevarlas al cuadrado antes de promediar, lo que lo hace sensible a outliers pero útil para cuantificar el impacto
energético de fallas. El Error Absoluto Medio (MAE) mide la desviación promedio en unidades originales, proporcionando una métrica robusta frente a errores aislados. Ambos indicadores permiten
evaluar el ajuste puntual de la línea base y comparar modelos bajo un mismo sistema de unidades.

3

6.2.

AIC y BIC

El Criterio de Información de Akaike (AIC) y el Criterio Bayesiano (BIC) equilibran bondad de ajuste
con parsimonia. Se definen como AIC = 2k −2 ln(L) y BIC = k ln(n)−2 ln(L), donde k es el número de
parámetros y L la verosimilitud. Valores menores indican modelos preferibles. En pronóstico industrial,
AIC se usa para explorar configuraciones con buen ajuste, mientras que BIC castiga más severamente
la complejidad, favoreciendo modelos fáciles de mantener en producción.

6.3.

Desviación Normalizada y Regresión sobre Desviaciones

La desviación normalizada zt = (yt − ŷt )/σres compara la observación con la predicción escalada por la
dispersión residual. Ajustar una regresión lineal sobre zt en una ventana móvil permite estimar la fuerza
de tendencia: pendientes positivas persistentes implican que la desviación se agrava, anticipando fallas.
Esta técnica combina el análisis de residuales de Box-Jenkins con indicadores de tendencia usados en
control estadístico de procesos.

6.4.

Desviación Media Absoluta y Escalado a Probabilidad

La desviación media absoluta sobre zt resume la magnitud promedio del alejamiento respecto a la
línea base. Al normalizar este valor al intervalo [0, 1] y combinarlo con la proporción de muestras fuera
de límites se obtiene un score interpretable como probabilidad de falla. El escalado puede realizarse
mediante funciones logísticas o transformaciones lineales calibradas con eventos históricos, manteniendo
trazabilidad sobre cómo cada componente aporta al riesgo final.

6.5.

Proporción Fuera de Límite y Fuerza de Tendencia

Los límites adaptativos se calculan a partir de la distribución de residuos (p.ej., percentiles 2.5 y 97.5).
La proporción de muestras que exceden dichos límites cuantifica la persistencia del evento anómalo.
Cuando este indicador se combina con la fuerza de tendencia (pendiente sobre zt ) se obtiene una visión
completa: magnitud, persistencia e inercia del desvío.

7.

Aprendizaje Incremental

7.1.

Teoría y Limitaciones

El aprendizaje incremental busca actualizar parámetros de un modelo a medida que llegan nuevos
datos, sin reentrenar desde cero. Desde la teoría de estimadores recursivos, una actualización ponderada
θt = αθt−1 + (1 − α)θlote corresponde a un filtro exponencial de primer orden [?]. Sus principales
limitaciones radican en la posible acumulación de sesgos si los nuevos lotes contienen ruido sistemático,
y en la necesidad de definir umbrales para gatillar reentrenamientos completos cuando las métricas de
desempeño se degradan. Las métricas monitoreadas (RMSE, MAE, ratio de anomalías, proporción
fuera de límites) actúan como señaladores de concept drift: incrementos sostenidos implican que la
distribución ha cambiado y que la actualización incremental ya no es suficiente.

8.

Conclusiones

La combinación de normalización textual, estandarización estadística, métricas multifactoriales, modelado temporal híbrido y aprendizaje incremental constituye un marco sólido para anticipar fallas

4

en sistemas eléctricos complejos. Cada técnica se sustenta en literatura verificada y se integra en un
pipeline gobernable capaz de traducir mediciones multivariadas en indicadores de riesgo auditables.

Referencias
[1] The Unicode Consortium. (2019). The Unicode Standard, Version 12.0. Disponible en: https:
//www.unicode.org/versions/Unicode12.0.0/
[2] Gray, J., & Reuter, A. (1992). Transaction Processing: Concepts and Techniques. Morgan Kaufmann.
[3] Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825–2830.
[4] Peña, D. (2002). Análisis de datos multivariantes. McGraw-Hill.
[5] Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). Time Series Analysis:
Forecasting and Control (5.ª ed.). Wiley.
[6] Taylor, S. J., & Letham, B. (2017). Forecasting at scale. The American Statistician, 72(1), 37–45.
[7] Liu, F. T., Ting, K. M., & Zhou, Z.-H. (2008). Isolation Forest. Proc. ICDM, 413–422.
[8] Montgomery, D. C., Jennings, C. L., & Kulahci, M. (2015). Introduction to Time Series Analysis
and Forecasting (2.ª ed.). Wiley.

5

