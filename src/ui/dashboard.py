import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import sys
import os
from pathlib import Path

# Inyectar ruta raÃ­z para resolver importaciones industriales
root_path = str(Path(__file__).resolve().parents[2])
if root_path not in sys.path:
    sys.path.append(root_path)

from src.config import SystemConfig
from src.data.preprocessor import DataPreprocessor
from src.features.selector import KeyVariableSelector
from src.models.baseline_modeler import BaselineModeler
from src.models.predictor import IndustrialFailurePredictor
from src.data.db_manager import DatabaseManager
import os
from datetime import datetime
from typing import Dict
from typing import Any

# Estilo Premium
st.set_page_config(page_title="Prognosis II Dashboard", layout="wide", page_icon="ðŸ“ˆ")

st.markdown("""
    <style>
    .main {
        background-color: #0e1117;
    }
    .stMetric {
        background-color: #1f2937;
        padding: 20px;
        border-radius: 10px;
        border: 1px solid #374151;
    }
    </style>
    """, unsafe_allow_html=True)

# InicializaciÃ³n de Core
@st.cache_resource
def init_engine():
    config = SystemConfig()
    db = DatabaseManager(config)
    # Asegurar esquema MLOps
    db.ensure_database_exists()
    db.setup_mlops_schema()
    
    preprocessor = DataPreprocessor(db)
    selector = KeyVariableSelector(config, db)
    modeler = BaselineModeler(config, db)
    return config, db, preprocessor, selector, modeler

config, db, preprocessor, selector, modeler = init_engine()

if getattr(config, "USE_SQLITE", False):
    st.sidebar.info("ðŸ“ Modo SQLite local (sin PostgreSQL)")

# --- Sidebar ---
st.sidebar.title("ðŸ› ï¸ ConfiguraciÃ³n")
file_path = st.sidebar.text_input("Ruta Excel/CSV de Datos", "filtered_consolidated_data_cleaned.xlsx")
threshold = st.sidebar.slider("Umbral de Alerta (%)", 0, 100, int(config.ALERT_THRESHOLDS['warning']*100)) / 100
max_vars_to_model = st.sidebar.slider("MÃ¡x. variables a modelar (baseline)", 3, 30, 15)
include_monitoring = st.sidebar.checkbox("Incluir variables en observaciÃ³n (monitoring)", value=True)

# Aplicar umbral WARNING desde UI (para alertas/reporte)
config.ALERT_THRESHOLDS['warning'] = float(threshold)

if st.sidebar.button("ðŸš€ Ejecutar Prognosis"):
    if os.path.exists(file_path):
        with st.spinner("Analizando seÃ±ales industriales (SimetrÃ­a Diamante)..."):
            # 1. Cargar y Preprocesar
            raw_data = preprocessor.load_data(file_path)
            if raw_data.empty:
                st.error("El archivo estÃ¡ vacÃ­o o no se pudo leer correctamente.")
                st.stop()
            
            clean_data = preprocessor.clean_data(raw_data)
            if clean_data.empty:
                st.error("No quedaron datos vÃ¡lidos despuÃ©s de la limpieza. Verifique el formato del archivo.")
                st.stop()
                
            # 2. âœ… NormalizaciÃ³n PRIMERO (fit_mode=True para entrenamiento inicial)
            # 3. NormalizaciÃ³n y Persistencia (Manejo de Cold Start)
            try:
                # Intentar normalizar con parÃ¡metros existentes (ProducciÃ³n)
                normalized_data = preprocessor.normalize_data(clean_data, fit_mode=False)
            except RuntimeError:
                # Si falla (Cold Start), entrenar scaler con el lote actual
                st.warning("âš ï¸ Primer arranque detectado: Calibrando Scaler Z-Score con datos actuales...")
                normalized_data = preprocessor.normalize_data(clean_data, fit_mode=True)
                
            preprocessor.save_to_db(normalized_data, "normalized_data_table")
            
            # 3. âœ… SelecciÃ³n de Variables SOBRE DATOS NORMALIZADOS (como notebook)
            critical_vars = selector.select_critical_variables(normalized_data)

            # Extraer categorÃ­as del selector (si estÃ¡n disponibles)
            selector_scores = selector.results.get('variables', {})
            monitoring_vars = [v for v, d in selector_scores.items() if d.get('category') == 'monitoring']
            discarded_vars = [v for v, d in selector_scores.items() if d.get('category') == 'discarded']
            
            # Guardar resultados del selector para el dashboard
            st.session_state['selector_results'] = selector.results
            
            if not critical_vars:
                st.warning("No se identificaron variables con anomalÃ­as claras. Usando Top 5 de mayor varianza para el diagnÃ³stico.")
                # SelecciÃ³n manual si el motor es muy estricto
                numeric_cols = normalized_data.select_dtypes(include=[np.number]).columns
                # Excluir timestamp/fecha
                numeric_cols = [c for c in numeric_cols if c not in ['timestamp', 'fecha', 'date', 'datetime']]
                variances = normalized_data[numeric_cols].var().sort_values(ascending=False)
                critical_vars = variances.head(5).index.tolist()
                monitoring_vars = []
                discarded_vars = []
            
            # Variables a modelar (paridad con notebook: incluir tambiÃ©n observaciÃ³n si se solicita)
            variables_to_model = list(critical_vars)
            if include_monitoring:
                # mantener orden determinista y no duplicar
                for v in monitoring_vars:
                    if v not in variables_to_model:
                        variables_to_model.append(v)

            # LIMITACIÃ“N INDUSTRIAL: no modelar mÃ¡s de N variables en UI
            if len(variables_to_model) > max_vars_to_model:
                st.info(
                    f"Se detectaron {len(critical_vars)} crÃ­ticas y {len(monitoring_vars)} en observaciÃ³n. "
                    f"Optimizando visualizaciÃ³n/modelado a {max_vars_to_model} variables."
                )
                variables_to_model = variables_to_model[:max_vars_to_model]
                
            if not critical_vars:
                st.error("No hay columnas numÃ©ricas disponibles para procesar.")
                st.stop()
            
            # 4. Modelado (Pipeline Granular con Feedback Visual)
            st.info(
                f"Iniciando modelado baseline de {len(variables_to_model)} variables "
                f"(crÃ­ticas={len(critical_vars)}, observaciÃ³n={len([v for v in variables_to_model if v in monitoring_vars])})..."
            )
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Usar timestamp como Ã­ndice si existe para mejorar Prophet/SARIMAX (agnÃ³stico al activo)
            if 'timestamp' in normalized_data.columns:
                normalized_indexed = normalized_data.set_index(pd.to_datetime(normalized_data['timestamp']))
            elif 'fecha' in normalized_data.columns:
                normalized_indexed = normalized_data.set_index(pd.to_datetime(normalized_data['fecha']))
            else:
                normalized_indexed = normalized_data

            for i, var in enumerate(variables_to_model):
                status_text.text(f"ðŸš€ Procesando {var} ({i+1}/{len(variables_to_model)})...")
                # El motor ya maneja la lÃ³gica de saltar modelos pesados si hay pocos datos
                series_for_model = normalized_indexed[var] if var in normalized_indexed.columns else normalized_data[var]
                results = modeler.fit_ensemble(var, series_for_model)
                modeler.save_baseline(results)
                progress_bar.progress((i + 1) / len(variables_to_model))
            
            status_text.empty()
            progress_bar.empty()
                
            # 5. AnÃ¡lisis de Probabilidad sobre TODOS los datos histÃ³ricos
            # âœ… CORRECCIÃ“N V11: El anÃ¡lisis debe hacerse sobre TODOS los datos histÃ³ricos
            # La historia es lo mÃ¡s importante para entender el comportamiento completo
            st.info(f"ðŸ“Š Calculando probabilidad sobre todos los datos histÃ³ricos: {len(normalized_data)} registros")
            
            predictor = IndustrialFailurePredictor(config, modeler)
            prediction = predictor.predict(normalized_data)  # âœ… TODOS los datos histÃ³ricos
            
            # 6. Generar Forecast de prÃ³ximas 24 horas (nuevo)
            # âœ… MEJORA V11: Forecast futuro usando modelos entrenados
            forecast_horizon_hours = 24
            forecast_result = predictor.generate_forecast(forecast_horizon_hours=forecast_horizon_hours)
            prediction['forecast'] = forecast_result  # Agregar forecast a la predicciÃ³n
            
            st.session_state['results'] = {
                'timestamp': prediction.get('timestamp', datetime.now().isoformat()),
                'health': prediction['system_health'],
                'influencers': prediction['top_influencers'], # Lista de tuples (var, risk, explanation)
                'shap_explanations': prediction.get('shap_explanations', []),  # âœ… Explicaciones SHAP
                'variable_ttf': prediction.get('variable_ttf', {}),  # âœ… TTF por variable
                'alerts': prediction.get('alerts', []),  # âœ… Alertas con TTF
                'variable_risks': prediction.get('variable_risks', {}),  # âœ… Probabilidad por variable (paridad notebook)
                'data': normalized_data,
                'mapping': preprocessor.quality_report['variable_mapping'],
                'critical_vars': critical_vars,  # Variables crÃ­ticas identificadas (selector)
                'monitoring_vars': monitoring_vars,  # Variables en observaciÃ³n (selector)
                'discarded_vars': discarded_vars,  # Variables descartadas (selector)
                'variables_modeled': variables_to_model,  # Variables para las que hay baseline
                'baseline_limits': modeler.results.get('adaptive_limits', {})  # LÃ­mites adaptativos usados
            }
            st.success("AnÃ¡lisis completado con Rigor Extremo.")
    else:
        st.error(f"Archivo no encontrado en {file_path}")

# --- Dashboard ---
st.title("ðŸ›¡ï¸ Prognosis II - Industrial Health Monitor")

if 'results' in st.session_state:
    res = st.session_state['results']

    def _risk_level(prob: float) -> str:
        if prob >= config.ALERT_THRESHOLDS['critical']:
            return 'CRITICAL'
        if prob >= config.ALERT_THRESHOLDS['warning']:
            return 'WARNING'
        return 'NORMAL'

    def _build_failure_report() -> str:
        ts = res.get('timestamp', datetime.now().isoformat())
        sys_prob = float(res['health'].get('probability', 0.0))
        sys_status = str(res['health'].get('status', 'normal')).upper()

        lines = [
            "",
            "=== REPORTE DE PREDICCIÃ“N DE FALLAS ===",
            "",
            f"Fecha: {ts}",
            "",
            "Estado del Sistema:",
            f"- Probabilidad: {sys_prob:.1%}",
            f"- Estado: {sys_status}",
            "",
            "Variables (ordenadas por probabilidad):"
        ]

        var_risks = res.get('variable_risks', {}) or {}
        for var, p in sorted(var_risks.items(), key=lambda x: x[1], reverse=True):
            lines.append(f"- {var}: {float(p):.1%} ({_risk_level(float(p))})")

        alerts = res.get('alerts') or []
        if alerts:
            lines.append("")
            lines.append("Alertas Activas:")
            for a in alerts:
                lines.append(f"- {a.get('level', '')}: {a.get('message', '')} (Prob: {float(a.get('probability', 0.0)):.1%})")

        return "\n".join(lines)
    
    # --- SECCIÃ“N 1: CABECERA EJECUTIVA (Zero Scroll) ---
    
    # 1.A KPIs Clave
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        prob = res['health']['probability']
        st.metric("Riesgo Global", f"{prob*100:.1f}%", help="Probabilidad combinada de falla del sistema")
    with col2:
        status = res['health']['status'].upper()
        color = "ðŸŸ¢" if status == "NORMAL" else "ðŸŸ¡" if status == "WARNING" else "ðŸ”´"
        st.metric("Estado Operativo", f"{color} {status}")
    with col3:
        ttf_hours = res['health'].get('ttf_hours')
        if ttf_hours is not None:
            if ttf_hours < 24:
                ttf_display = f"{ttf_hours:.1f}h âš ï¸"
            elif ttf_hours < 72:
                ttf_display = f"{ttf_hours/24:.1f} dÃ­as"
            else:
                ttf_display = f"{ttf_hours/24:.0f} dÃ­as"
            st.metric("Time-to-Failure Est.", ttf_display, delta_color="inverse")
        else:
            st.metric("Salud Estimada", f"{100 - prob*100:.1f}%")
    with col4:
        active_alerts = len(res.get('alerts', []))
        st.metric("Alertas Activas", active_alerts, delta="CrÃ­ticas" if any(a['level']=='CRITICAL' for a in res.get('alerts', [])) else "Normal", delta_color="inverse")

    # 1.B Alertas CrÃ­ticas (Solo si existen, banner colapsable pero visible por defecto si es crÃ­tico)
    alerts = res.get('alerts', [])
    critical_alerts = [a for a in alerts if a['level'] == 'CRITICAL']
    if critical_alerts:
        st.error(f"ðŸš¨ **ATENCIÃ“N: {len(critical_alerts)} Alertas CrÃ­ticas Detectadas**")
        with st.expander("Ver Alertas CrÃ­ticas", expanded=True):
            for alert in critical_alerts:
                st.markdown(f"**ðŸ”´ {alert['variable']}**: {alert['message']}")

    # 1.C GrÃ¡fico Maestro (Risk ECG)
    st.markdown("### ðŸ“‰ Tendencia Global de Riesgo")
    # LÃ³gica de grÃ¡fico (simplificada para vista ejecutiva)
    
    # Preparar datos para ECG
    granularity = st.selectbox(
        "Horizonte de tiempo:",
        ["Ãšltimas 24 horas", "Todos los datos", "Ãšltimos 7 dÃ­as"],
        key="ecg_granularity_exec"
    )
    
    data_df = res.get('data', pd.DataFrame()).copy()
    limits_map = res.get('baseline_limits', {}) or {}
    modeled_vars = [v for v in (res.get('variables_modeled') or []) if v in data_df.columns and v in limits_map]
    
    if 'timestamp' in data_df.columns and modeled_vars:
        x_ts = pd.to_datetime(data_df['timestamp'], errors='coerce')
        order = np.argsort(x_ts.values)
        x_ts = x_ts.iloc[order]
        df_sorted = data_df.iloc[order]
        
        if granularity == "Ãšltimas 24 horas":
             mask = x_ts >= (x_ts.max() - pd.Timedelta(hours=24))
             x_ts, df_sorted = x_ts[mask], df_sorted[mask]
        elif granularity == "Ãšltimos 7 dÃ­as":
             mask = x_ts >= (x_ts.max() - pd.Timedelta(days=7))
             x_ts, df_sorted = x_ts[mask], df_sorted[mask]

        # Calcular prob histÃ³rica (versiÃ³n optimizada para display)
        probs_by_var = []
        for v in modeled_vars[:15]: # Limitado a 15 para velocidad en render
            s = pd.to_numeric(df_sorted[v], errors='coerce')
            lim = limits_map[v]
            base, up, low = float(lim.get('baseline',0)), float(lim.get('upper',0)), float(lim.get('lower',0))
            rng = max(up - low, 1e-9)
            
            # CÃ¡lculo vectorizado rÃ¡pido
            dev = ((s - base) / rng).abs().clip(0,1)
            out = ((s > up) | (s < low)).astype(float)
            probs_by_var.append((0.4 * dev + 0.4 * out + 0.2 * 0.0).clip(0,1)) # Trend simplificado 0 para velocidad visual
            
        if probs_by_var:
            system_p = np.mean(np.vstack(probs_by_var), axis=0) if len(probs_by_var) > 0 else []
            fig_ecg = go.Figure()
            fig_ecg.add_trace(go.Scatter(x=x_ts, y=system_p, mode='lines', name='Riesgo HistÃ³rico', line=dict(color='#ef4444', width=2)))
            
            # Forecast (si existe)
            forecast = res.get('forecast', {})
            if forecast and 'forecast_timestamps' in forecast:
                 ft_ts = pd.to_datetime(forecast['forecast_timestamps'])
                 fig_ecg.add_trace(go.Scatter(x=ft_ts, y=[forecast['system_forecast_prob']]*len(ft_ts), mode='lines', name='ProyecciÃ³n', line=dict(color='#f59e0b', dash='dash')))

            fig_ecg.update_layout(template="plotly_dark", height=300, margin=dict(l=20, r=20, t=10, b=10), yaxis_range=[0,1], title_text="")
            st.plotly_chart(fig_ecg, width="stretch")
    else:
        st.info("No hay datos temporales suficientes para mostrar la tendencia.")

    # --- SECCIÃ“N 2: DIVULGACIÃ“N PROGRESIVA (Expanders) ---

    # 2.A AnÃ¡lisis Detallado
    with st.expander("ðŸ” AnÃ¡lisis de Causa RaÃ­z (Variables CrÃ­ticas)", expanded=False):
        c1, c2 = st.columns([1, 2])
        with c1:
            st.markdown("#### Top Influenciadores (XAI)")
            st.info(
                "**Â¿QuÃ© es XAI?** (Explainable AI)\n"
                "A diferencia de una 'Caja Negra', Prognosis desglosa el riesgo en factores fÃ­sicos:\n"
                "- **DesviaciÃ³n:** QuÃ© tan lejos estÃ¡ de lo normal.\n"
                "- **Estabilidad:** QuÃ© tan errÃ¡tica es la seÃ±al.\n"
                "- **Tendencia:** QuÃ© tan rÃ¡pido se deteriora."
            )
            if res['influencers']:
                influ_data = [{'Variable': i[0], 'Aporte al Riesgo': i[1]} for i in res['influencers'][:10] if len(i)>=2]
                st.dataframe(pd.DataFrame(influ_data).set_index('Variable').style.background_gradient(cmap='Reds'), width="stretch")
        
        with c2:
            st.markdown("#### Top Variables en Riesgo (GrÃ¡ficas con LÃ­mites)")
            top5 = sorted(res.get('variable_risks', {}).items(), key=lambda x: x[1], reverse=True)[:3]
            
            limits_map = res.get('baseline_limits', {})
            
            for var_name, p in top5:
                if var_name in data_df.columns:
                    # Preparar datos
                    series_data = data_df[var_name].tail(100)
                    if 'timestamp' in data_df.columns:
                        timestamps = data_df['timestamp'].tail(100)
                    else:
                        timestamps = range(len(series_data))
                    
                    fig_var = go.Figure()
                    
                    # Serie Real
                    fig_var.add_trace(go.Scatter(
                        x=timestamps, y=series_data, 
                        mode='lines', name='Valor Real',
                        line=dict(color='#3b82f6', width=2)
                    ))
                    
                    # LÃ­mites Baseline
                    if var_name in limits_map:
                        lims = limits_map[var_name]
                        upper = float(lims.get('upper', 0))
                        lower = float(lims.get('lower', 0))
                        
                        fig_var.add_hline(y=upper, line_dash="dash", line_color="red", annotation_text="LÃ­mite Sup.")
                        fig_var.add_hline(y=lower, line_dash="dash", line_color="red", annotation_text="LÃ­mite Inf.")
                    
                    fig_var.update_layout(
                        title=f"{var_name} (Prob: {p:.1%})",
                        template="plotly_dark",
                        height=250,
                        margin=dict(l=20, r=20, t=30, b=20),
                        xaxis_title="Tiempo",
                        yaxis_title="Valor"
                    )
                    st.plotly_chart(fig_var, width="stretch")

    # Pre-calcular reporte para uso en descarga y visualizaciÃ³n
    report_text = _build_failure_report()
    
    # 2.B Tabla de Datos Completa
    with st.expander("ðŸ“Š Tabla de Probabilidades y Estado (Detalle TTF)", expanded=False):
        var_risks = res.get('variable_risks', {})
        if var_risks:
            # Enriquecer tabla con explicaciÃ³n de TTF
            st.caption(
                "**Nota sobre TTF (Time-to-Failure):** "
                "Calculado proyectando la velocidad de degradaciÃ³n actual ($dx/dt$) hacia el lÃ­mite crÃ­tico. "
                "Una tendencia pronunciada puede reducir drÃ¡sticamente el TTF aunque el valor actual estÃ© lejos del lÃ­mite."
            )
            full_df = pd.DataFrame([
                {'Variable': k, 
                 'Probabilidad': v, 
                 'Estado': _risk_level(v), 
                 'TTF (h)': res.get('variable_ttf',{}).get(k,{}).get('ttf_hours')}
                for k,v in var_risks.items()
            ]).sort_values('Probabilidad', ascending=False)
            
            # Formatear columnas
            st.dataframe(
                full_df.style.format({'Probabilidad': '{:.1%}', 'TTF (h)': '{:.1f}'})
                .background_gradient(subset=['Probabilidad'], cmap='Reds'),
                width="stretch"
            )
            
            # Descarga
            st.download_button("â¬‡ï¸ Descargar Reporte TXT", report_text, "reporte_falla.txt")

    # 2.C DiagnÃ³stico TÃ©cnico
    with st.expander("ðŸ› ï¸ DiagnÃ³stico TÃ©cnico (Variables Seleccionadas)", expanded=False):
        c_diag1, c_diag2 = st.columns(2)
        with c_diag1:
            st.markdown("**Variables CrÃ­ticas (Modeladas)**")
            crit_vars = res.get('critical_vars', [])
            st.dataframe(pd.DataFrame(crit_vars, columns=["Variable"]), height=200, width="stretch")
            st.caption(f"Total: {len(crit_vars)}")
            
        with c_diag2:
            st.markdown("**Variables en ObservaciÃ³n (Monitoring)**")
            mon_vars = res.get('monitoring_vars', [])
            st.dataframe(pd.DataFrame(mon_vars, columns=["Variable"]), height=200, width="stretch")
            st.caption(f"Total: {len(mon_vars)}")

        st.divider()
        st.markdown("**Reporte Crudo del Motor (Logs)**")
        with st.container(height=300):
            st.code(report_text, language="text")

    # 9. EvaluaciÃ³n cientÃ­fica (backtest temporal) - incremental / datos reales
    with st.expander("ðŸ§ª EvaluaciÃ³n cientÃ­fica (backtest temporal)"):
        st.caption(
            "Rigor cientÃ­fico: para evaluar forecasting/anomalÃ­a necesitas comparar predicciÃ³n vs datos futuros (holdout). "
            "Este backtest ejecuta una validaciÃ³n temporal simple sobre las variables top por riesgo."
        )

        horizon = st.number_input("Horizonte holdout (puntos)", min_value=6, max_value=168, value=24, step=6)
        top_k_eval = st.number_input("NÃºmero de variables a evaluar (top por riesgo)", min_value=1, max_value=10, value=3, step=1)
        run_eval = st.button("â–¶ï¸ Ejecutar backtest (puede tardar)")

        def _metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict[str, float]:
            y_true = np.asarray(y_true, dtype=float)
            y_pred = np.asarray(y_pred, dtype=float)
            rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))
            mae = float(np.mean(np.abs(y_true - y_pred)))
            mape = float(np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100.0)
            return {"rmse": rmse, "mae": mae, "mape_%": mape}

        if run_eval and var_risks:
            from statsmodels.tsa.statespace.sarimax import SARIMAX
            eval_rows = []
            candidates = [v for v, _ in sorted(var_risks.items(), key=lambda x: x[1], reverse=True)[: int(top_k_eval)]]

            for v in candidates:
                if v not in res['data'].columns:
                    continue
                s = res['data'][v].dropna()
                if len(s) < int(horizon) + 20:
                    continue

                train = s.iloc[: -int(horizon)]
                test = s.iloc[-int(horizon) :]

                # 1) Naive robusto: mediana del train
                naive_pred = np.full(shape=len(test), fill_value=float(train.median()))
                naive_m = _metrics(test.values, naive_pred)

                # 2) SARIMAX (si tenemos parÃ¡metros guardados desde el ensamble)
                sarima_m = None
                sarima_metrics = None
                try:
                    v_models = modeler.models.get(v, {})
                    sarima_metrics = v_models.get("sarima_metrics", {})
                    order = sarima_metrics.get("order")
                    seasonal_order = sarima_metrics.get("seasonal_order")

                    if order is not None and seasonal_order is not None:
                        m_sar = SARIMAX(
                            train,
                            order=tuple(order),
                            seasonal_order=tuple(seasonal_order),
                            enforce_stationarity=False,
                            enforce_invertibility=False,
                        )
                        fitted = m_sar.fit(disp=False, low_memory=True, maxiter=50)
                        pred = fitted.forecast(steps=len(test))
                        sarima_m = _metrics(test.values, np.asarray(pred))
                except Exception:
                    sarima_m = None

                row = {
                    "variable": v,
                    "risk_now": float(var_risks.get(v, 0.0)),
                    "naive_rmse": naive_m["rmse"],
                    "naive_mae": naive_m["mae"],
                    "naive_mape_%": naive_m["mape_%"],
                }
                if sarima_m:
                    row.update(
                        {
                            "sarimax_rmse": sarima_m["rmse"],
                            "sarimax_mae": sarima_m["mae"],
                            "sarimax_mape_%": sarima_m["mape_%"],
                            "sarimax_aic_fullfit": (sarima_metrics or {}).get("aic"),
                        }
                    )
                eval_rows.append(row)

            if eval_rows:
                st.dataframe(pd.DataFrame(eval_rows).sort_values("risk_now", ascending=False), width="stretch")
                st.caption(
                    "InterpretaciÃ³n: si SARIMAX mejora RMSE/MAE vs naive, la componente de forecasting aporta valor predictivo. "
                    "Si no mejora, el riesgo se estÃ¡ explicando mÃ¡s por lÃ­mites/anomalÃ­a que por forecasting."
                )
            else:
                st.warning("No se pudo ejecutar backtest (insuficientes datos por variable o faltan parÃ¡metros SARIMAX).")
else:
    st.info("ðŸ‘ˆ Configure la ruta de datos y presione 'Ejecutar Prognosis' para comenzar el diagnÃ³stico.")

st.divider()
st.caption(f"Audit Status: Diamond Standard (Certified) | Architecture: Modular MLOps | {datetime.now().strftime('%Y-%m-%d %H:%M')}")
